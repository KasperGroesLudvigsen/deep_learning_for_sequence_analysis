{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5d63b-ea03-48a6-9762-3ee73b9e8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run\n",
    "\n",
    "params = {\n",
    "    \"experiment_desc\" : \"BIGGER PUNC w. 100k vocab\", # Used to keep track of my different experiments\n",
    "    \"experiment_name\" : \"experiment32\", # Used to keep track of my different experiments\n",
    "    \"interval\" : (0, 45), # includes sentences of length longer than 0th index, shorter than 1st index\n",
    "    \"test_size\" : 0.2,\n",
    "    \"validation_size\" : 0.2,\n",
    "    \"batchsize\" : 128, # Xu et al 2016: 128\n",
    "    \"token_type\" : \"lemma\", # should be \"class\" for word class or \"lemma\" for word lemma\n",
    "    \"corpus\" : \"1990\", # Dont change\n",
    "    \"skip_no_comma\" : False, # if True, sentences that do not contain at least 1 comma are discarded\n",
    "    \"comma_representation\" : \",\", # Dont change\n",
    "    \"max_norm\" : 5,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"weight_decay\" : 0.0,\n",
    "    \"patience\" : 10, # if no improvement in validation loss after 'patience' epochs, training is terminated\n",
    "    \"epochs\" : 200,\n",
    "    \"num_embeddings\" : 100003, # number of vectors in the embedding. Should correspond to number of unique tokens in vocabulary\n",
    "    \"embedding_dim\" : 256, # length of each word vector in the embedding\n",
    "    \"hidden_size\" : 1024, # hidden_size argument in LSTM\n",
    "    \"loss\" : \"cel\", # should either be \"focal\" or \"cel\"\n",
    "    \"lstm_layers\" : 5,\n",
    "    \"big_punc\" : True,\n",
    "    \"num_unique_words\" : 100000 # must always be supplied, but is only used if token_type is \"lemma\". Must be either 8000, 50000, 100000 or 200000\n",
    "    }\n",
    "\n",
    "print(params[\"experiment_name\"])\n",
    "run.run_experiment(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
